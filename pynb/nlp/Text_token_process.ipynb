{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b7057b-2394-443e-9524-33bd35dee274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d0606-043a-4a6d-964d-2d8bec5a673d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb69b78-b8c4-4927-9849-56e45c089351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ed8571-4354-4051-8638-7c5c2dd75dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be our corpus which we will work on\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31e825d-a272-4ec8-b4a1-805a70aa5371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run  times !!\n"
     ]
    }
   ],
   "source": [
    "#removing digits in the corpus\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adbc5b8-97e3-484e-8b32-6a054ec116b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to finalize the demo corpus which will be used for this notebook  should be done soon  It should be done by the ending of this month But will it This notebook has been run  times \n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ce87db-15ea-477e-9d7f-e0ece3d1fb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Need to finalize the demo corpus which will be used for this notebook should be done soon It should be done by the ending of this month But will it This notebook has been run times'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a4d58cc-7bd0-4310-865a-3d724dce51f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_nltk = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bd5c4cc-e764-4d9f-b748-842f33f6ddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i'],\n",
       " [],\n",
       " ['l'],\n",
       " [],\n",
       " ['v'],\n",
       " ['e'],\n",
       " [],\n",
       " [],\n",
       " ['u'],\n",
       " ['n'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " [],\n",
       " ['h'],\n",
       " [],\n",
       " ['p'],\n",
       " ['e'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['u'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_stops_digits(tokens):\n",
    "    return [ t.lower() for t in tokens if t not in stop_words_nltk and not t.isdigit() and t not in string.punctuation ]\n",
    "    \n",
    "def preprocess_corpus(texts):\n",
    "    return [ remove_stops_digits( word_tokenize(text)) for text in texts ]\n",
    "   \n",
    "test_stopwords =     preprocess_corpus( \"I love sundays No 1, and hope you too\" )\n",
    "test_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686c1929-dcab-4acb-9840-8d7c0118227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'But', 'will', 'it', 'This', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['Need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'It', 'done', 'ending', 'month', 'But', 'This', 'notebook', 'run', 'times']\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a6720cb-9d8d-4bbe-a41b-fcfc5b3fbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SPACY \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "###  python -m spacy download en\n",
    "spacy_model = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "697821f9-f647-4dd9-abb7-9c74af6eae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy:\n",
      "Tokenized Corpus: ['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'But', 'will', 'it', 'This', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords ['Need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'It', 'ending', 'month', 'But', 'This', 'notebook', 'run', 'times']\n",
      "Difference between NLTK and spaCy output:\n",
      " {'used', 'done'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "\n",
    "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
    "\n",
    "\n",
    "print(\"Difference between NLTK and spaCy output:\\n\",\n",
    "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9512363-e879-46fe-904f-d4e700a9c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "Need to finalize the demo corpus which will be used for this notebook should be done soon It should be done by the ending of this month But will it This notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db38cc28-a044-4f80-8615-960a1531b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to finalize the demo corpus which will be used for this notebook should be done soon It should be done by the ending of this month But will it This notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b05cbb14-72ae-42ac-9f90-32f83958d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : NOUN\n",
      "corpus : X\n",
      "which : PRON\n",
      "will : AUX\n",
      "be : AUX\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : AUX\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : AUX\n",
      "been : AUX\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      "POS Tagging using NLTK:\n",
      "[('Need', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('finalize', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('demo', 'NN'),\n",
      " ('corpus', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('will', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('used', 'VBN'),\n",
      " ('for', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('and', 'CC'),\n",
      " ('it', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('soon', 'RB'),\n",
      " ('!', '.'),\n",
      " ('!', '.'),\n",
      " ('.', '.'),\n",
      " ('It', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('ending', 'VBG'),\n",
      " ('of', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('month', 'NN'),\n",
      " ('.', '.'),\n",
      " ('But', 'CC'),\n",
      " ('will', 'MD'),\n",
      " ('it', 'PRP'),\n",
      " ('?', '.'),\n",
      " ('This', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('been', 'VBN'),\n",
      " ('run', 'VBN'),\n",
      " ('4', 'CD'),\n",
      " ('times', 'NNS'),\n",
      " ('!', '.'),\n",
      " ('!', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS tagging using spacy\n",
    "print(\"POS Tagging using spacy:\")\n",
    "doc = spacy_model(corpus_original)\n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token,\":\", token.pos_)\n",
    "\n",
    "#pos tagging using nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(\"POS Tagging using NLTK:\")\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7242adf-9dcd-4887-afb2-98d482208ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb8b0672-dd3a-4151-857a-259a77ccb2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1f1668-5af1-4010-85c3-464f49015bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.', 'Strange loves pav bhaji of mumbai.', 'Hulk loves chat of delhi']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98bd177b-5e03-4708-a9c6-282248a66ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr',\n",
       " '.',\n",
       " 'Strange',\n",
       " 'loves',\n",
       " 'pav',\n",
       " 'bhaji',\n",
       " 'of',\n",
       " 'mumbai',\n",
       " '.',\n",
       " 'Hulk',\n",
       " 'loves',\n",
       " 'chat',\n",
       " 'of',\n",
       " 'delhi']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "157bc7ed-db30-4677-8a7c-25e8d6182623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos= 'a' ))  ##  a is for adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fafec5f7-0d50-4cea-bcfd-8ef1131c4901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "token = nlp(u'better')\n",
    "for word in token:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe11b9-b419-4bee-9795-e51051862d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
